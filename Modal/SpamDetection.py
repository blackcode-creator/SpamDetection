# -*- coding: utf-8 -*-
"""SpamDetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kvOSCM4AdiM0STM8x2_5kpJtiCrQrw95
"""

#Loading libraries
import numpy as np
import pandas as pd
from google.colab import files
from nltk.stem import PorterStemmer
import nltk
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder
import seaborn as sns
from wordcloud import WordCloud
from nltk.corpus import stopwords
import string
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB
from sklearn.metrics import accuracy_score,confusion_matrix,precision_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.ensemble import StackingClassifier
import pickle

# Importing the dataset from the local computer to upload
print("Upload your csv dataset\n")
uploaded = files.upload()

# Loading the csv dataset and encoding it to ISO-8859-1
df = pd.read_csv('spam.csv', encoding = "ISO-8859-1")

df.sample(5)

# About amount of data in the spam.csv
df.shape

#1. DATA CLEANING
#2. EDA
#3. Text Preprocessing 
#4. Model Building 
#5. Evaluation
#6. Improvement
#7. Website

"""# **Step 1. Data Cleaning**"""

df.info()

# Dropping the 3 columns to clean the datasets
df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'], inplace=True)

# After cleaaning check for our data
df.sample(5)

# Renaming the columns 
df.rename(columns={'v1':'target','v2':'text'},inplace=True)
df.sample(5)

#Data preprocessing by encoding it
encoder = LabelEncoder()

df['target'] = encoder.fit_transform(df['target'])

df.head()

# Checking for missing values
df.isnull().sum()

#Check for duplicate values
df.duplicated().sum()

#Remove the duplicates
df = df.drop_duplicates(keep='first')

#Check the duplicate if they have been removed 
df.duplicated().sum()

#After data cleaning check the total of data has reduced to 5169 from 5572 we have remove 403
df.shape

"""# **Step 2. EDA**"""

df.head()

df['target'].value_counts()

plt.pie(df['target'].value_counts(), labels=['ham','spam'],autopct="%0.2f")
plt.show()

#Dataset is imbalanced
nltk.download('punkt')

df['num_characters'] = df['text'].apply(len)
df.head()

#Number of words
df['num_words'] = df['text'].apply(lambda x: len(nltk.word_tokenize(x)))
df.head()

#Number of sentences per row
df['num_sentences'] = df['text'].apply(lambda x: len(nltk.sent_tokenize(x)))
df.head()

#Displaying the statistics of the spam.csv 
df[['num_characters','num_words','num_sentences']].describe()

#Describing the Ham message
df[df['target'] == 0][['num_characters','num_words','num_sentences']].describe()

#Describing the Spam message
df[df['target'] == 1][['num_characters','num_words','num_sentences']].describe()

# Using seaborn to plot histogram for number of characters
plt.figure(figsize=(12,6))
sns.histplot(df[df['target'] == 0]['num_characters'])
sns.histplot(df[df['target'] == 1]['num_characters'],color='red')

# Using seaborn to plot histogram for no of word per message
plt.figure(figsize=(12,6))
sns.histplot(df[df['target'] == 0]['num_words'])
sns.histplot(df[df['target'] == 1]['num_words'],color='red')

#Display the relationship of the target which consist of no of characters, words and sentences
sns.pairplot(df,hue='target')

#displaying the correlation of the model
sns.heatmap(df.corr(),annot=True)

ps = PorterStemmer()
ps.stem('loving')

# Data Preprocessing

# Download stopwords
nltk.download('stopwords')


stopwords.words('english')

#Looking at punctuation
string.punctuation  

 
def transform_text(text):
  text = text.lower() # 1. Lowering the case of text
  text = nltk.word_tokenize(text) # 2. Tokenization
  y = []
  for i in text:  #Removing special characters
    if i.isalnum():
      y.append(i)

  text = y[:]
  y.clear()
  for i in text:
    if i not in stopwords.words('english') and i not in string.punctuation: #validates the text if they have unsual characters
      y.append(i)

  text = y[:]
  y.clear()

  for i in text:
    y.append(ps.stem(i))
    
  return " ".join(y)

# Passing a sentences to vaildate if the validate is okay
transform_text('But ill b going 2 sch on mon. My sis need 2 take smth.')

df['text'][2000]

df['transformed_text'] = df['text'].apply(transform_text)

#View after text is transformed

df.head()

wc = WordCloud(width=500, height = 500, min_font_size=10,background_color='white')

spam_wc = wc.generate(df[df['target'] == 1]['transformed_text'].str.cat(sep= " "))

plt.figure(figsize=(15,6))
plt.imshow(spam_wc)

ham_wc = wc.generate(df[df['target'] == 0]['transformed_text'].str.cat(sep= " "))

plt.figure(figsize=(15,6))
plt.imshow(ham_wc)

df.head()

# Tranformed_text which are spam
spam_corpus = []
for msg in df[df['target'] == 1]['transformed_text'].tolist():
  for word in msg.split():
    spam_corpus.append(word)

spam_corpus

len(spam_corpus)

sns.barplot(pd.DataFrame(Counter(spam_corpus).most_common(30))[0],pd.DataFrame(Counter(spam_corpus).most_common(30))[1])
plt.xticks(rotation = 'vertical')
plt.show()

# Tranformed_text which are ham
ham_corpus = []
for msg in df[df['target'] == 0]['transformed_text'].tolist():
  for word in msg.split():
    ham_corpus.append(word)

len(ham_corpus)

sns.barplot(pd.DataFrame(Counter(ham_corpus).most_common(30))[0],pd.DataFrame(Counter(ham_corpus).most_common(30))[1])
plt.xticks(rotation = 'vertical')
plt.show()

# Text Vectorization
#Using bag of words
df.head()

#Modal building
cv = CountVectorizer()
tfidf = TfidfVectorizer(max_features=3000)
X = tfidf.fit_transform(df['transformed_text']).toarray()
#Display X
X.shape

#from sklearn.preprocessing import MinMaxScaler
#scaler = MinMaxScaler()
#X = scaler.fit_transform(X)

# appending the num_character col to X
#X = np.hstack((X,df['num_characters'].values.reshape(-1,1)))

y = df['target'].values

#Display y
y

#Testing the modal
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)

#Why use Naive Bayes
# It doesn't require as much training data. It handles both continuous and discrete data. 
# It is highly scalable with the number of predictors and data points. It is fast and can be used to make real-time predictions
gnb = GaussianNB()
mnb = MultinomialNB()
bnb = BernoulliNB()

#Using GaussianNB
gnb.fit(X_train,y_train)
y_pred1 = gnb.predict(X_test)
print(accuracy_score(y_test,y_pred1))
print(confusion_matrix(y_test,y_pred1))
print(precision_score(y_test,y_pred1))

#Using MultinomialNB()
mnb.fit(X_train,y_train)
y_pred2 = mnb.predict(X_test)
print(accuracy_score(y_test,y_pred2))
print(confusion_matrix(y_test,y_pred2))
print(precision_score(y_test,y_pred2))

#Using BernoulliNB()
bnb.fit(X_train,y_train)
y_pred3 = bnb.predict(X_test)
print(accuracy_score(y_test,y_pred3))
print(confusion_matrix(y_test,y_pred3))
print(precision_score(y_test,y_pred3))

svc = SVC(kernel='sigmoid', gamma=1.0)
knc = KNeighborsClassifier()
mnb = MultinomialNB()
dtc = DecisionTreeClassifier(max_depth=5)
lrc = LogisticRegression(solver='liblinear', penalty='l1')
rfc = RandomForestClassifier(n_estimators=50, random_state=2)
abc = AdaBoostClassifier(n_estimators=50, random_state=2)
bc = BaggingClassifier(n_estimators=50, random_state=2)
etc = ExtraTreesClassifier(n_estimators=50, random_state=2)
gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)
xgb = XGBClassifier(n_estimators=50,random_state=2)

clfs = {
    'SVC' : svc,
    'KN' : knc, 
    'NB': mnb, 
    'DT': dtc, 
    'LR': lrc, 
    'RF': rfc, 
    'AdaBoost': abc, 
    'BgC': bc, 
    'ETC': etc,
    'GBDT':gbdt,
    'xgb':xgb
}

def train_classifier(clf,X_train,y_train,X_test,y_test):
    clf.fit(X_train,y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test,y_pred)
    precision = precision_score(y_test,y_pred)
    
    return accuracy,precision

train_classifier(svc,X_train,y_train,X_test,y_test)

accuracy_scores = []
precision_scores = []

for name,clf in clfs.items():
    
    current_accuracy,current_precision = train_classifier(clf, X_train,y_train,X_test,y_test)
    
    print("For ",name)
    print("Accuracy - ",current_accuracy)
    print("Precision - ",current_precision)
    
    accuracy_scores.append(current_accuracy)
    precision_scores.append(current_precision)

performance_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False)

#Display the performance
performance_df

#Labeling the algorithms
performance_df1 = pd.melt(performance_df, id_vars = "Algorithm")

#Display the performance
performance_df1

#Ploting the graphical visualization of the algorithms
sns.catplot(x = 'Algorithm', y='value', hue = 'variable',data=performance_df1, kind='bar',height=5)
plt.ylim(0.5,1.0)
plt.xticks(rotation='vertical')
plt.show()

# model improve
# 1. Change the max_features parameter of TfIdf
temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_max_ft_3000':accuracy_scores,'Precision_max_ft_3000':precision_scores}).sort_values('Precision_max_ft_3000',ascending=False)
temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_scaling':accuracy_scores,'Precision_scaling':precision_scores}).sort_values('Precision_scaling',ascending=False)
new_df = performance_df.merge(temp_df,on='Algorithm')
new_df_scaled = new_df.merge(temp_df,on='Algorithm')
temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_num_chars':accuracy_scores,'Precision_num_chars':precision_scores}).sort_values('Precision_num_chars',ascending=False)
new_df_scaled.merge(temp_df,on='Algorithm')

# Voting Classifier
svc = SVC(kernel='sigmoid', gamma=1.0,probability=True)
mnb = MultinomialNB()
etc = ExtraTreesClassifier(n_estimators=50, random_state=2)

voting = VotingClassifier(estimators=[('svm', svc), ('nb', mnb), ('et', etc)],voting='soft')
voting.fit(X_train,y_train)

y_pred = voting.predict(X_test)
print("Accuracy",accuracy_score(y_test,y_pred))
print("Precision",precision_score(y_test,y_pred))

# Applying stacking
estimators=[('svm', svc), ('nb', mnb), ('et', etc)]
final_estimator=RandomForestClassifier()

clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)

clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
print("Accuracy",accuracy_score(y_test,y_pred))
print("Precision",precision_score(y_test,y_pred))

pickle.dump(tfidf,open('vectorizer.pkl','wb'))
pickle.dump(mnb,open('model.pkl','wb'))